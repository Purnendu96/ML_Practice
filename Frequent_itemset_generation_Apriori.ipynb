{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Machine Learning/Assignment_1 <br>MDS201803 <br>MDS201811<br>Purnendu Ghosh<br>Subhasish Basak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the code to find frequent itemsets using Apriori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each collection we are provided with 2 datasets viz. docword and vocab.<br>\n",
    "we follow these steps:\n",
    "- We store the 2 datasets using PANDAS dataframe as docword and vocab\n",
    "- From the docword dataframe we discard the first 3 header rows which cntain information about the dataset\n",
    "- Next we split the remaining portion of the docword dataset into 3 parts consisting docID, wordID and count\n",
    "- After that we create a dictionary to store the word_id's corresponding to each doc_id by iterating over the rows of the truncated dataset\n",
    "- Using this dictionary we contruct a array of arrays which stores the word_ids corresponding to each document\n",
    "- Now, to perform apriori we use the **\"mlxtend\"** library\n",
    "- the apriori function under mlxtend requires a Matrix and minimum support (user input F) as input. The matrix has the word_ids as columns and doc_ids as rows. Each element of the matrix stores either True or False according as the word_id is present in the doc_id. This matrix turns out to be a sparse matrix\n",
    "- Then we filter the frequent itemsets w.r.t the size of the set (user input K). \n",
    "- Next we create another dictionary to store the words corresponding to each word_id, using the vocab dataset.\n",
    "- Using this dictionary we map each word_id in the fequent itemset to the original word, which gives the required answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function takes 3 inputs:\n",
    "    - n : name of the collection\n",
    "    - K : size of the required frequent itemset\n",
    "    - F : minimum support\n",
    "Our function uses the following variabls: \n",
    "    - docword : Stores the docword dataset as pandas dataframe\n",
    "    - vocab : Stores the vocab dataset as pandas dataframe\n",
    "    - trunc : Stores the truncated docword dataset as pandas dataframe\n",
    "    - docword_splitted : Stores the splitted truncated docword dataset as pandas dataframe\n",
    "    - dict_1 : dictionary stores word ID's corresponding each document\n",
    "    - array : stores array of arrays containing all the inique word_id's of the documents\n",
    "    - Sp_matrix : stores the sparse matrix\n",
    "    - vocab_dict : dictionary stores words corresponding to word_id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Frequent_itemset(n,K,F):\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    from mlxtend.preprocessing import TransactionEncoder\n",
    "    from mlxtend.frequent_patterns import apriori\n",
    "    f=open(\"Frequent_Itemset_Output_nips.txt\", 'w')\n",
    "    start = time.time()\n",
    "    docword = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/docword.\"+n+\".txt.gz\",header=None)\n",
    "    vocab = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/vocab.\"+n+\".txt\",header=None)\n",
    "    vocab.columns = [\"Word\"] \n",
    "    trunc = docword[3:]        #truncates the Docword to remove the first 3 header line\n",
    "    docword_splitted = pd.DataFrame(trunc[0].str.split(\" \", n = 2, expand = True))    #Splits the truncated Docword into a data frame of 3 columns storing D,W,NNZ\n",
    "    docword_splitted.columns = ['docID', 'wordID', 'count']      # Renames the column names of the splitted dataframe\n",
    "    dict_1 = docword_splitted.groupby('docID')['wordID'].apply(list).to_dict()      #creates dictionary for document ID's to store word ID's corresponding each document\n",
    "    array = list(dict_1.values())                             #creates an array of arrays containing all the inique word_id's of the documents\n",
    "    Sp = TransactionEncoder()        \n",
    "    Sp_array = Sp.fit(array).transform(array)\n",
    "    Sp_matrix = pd.DataFrame(Sp_array, columns=Sp.columns_)   #creates the sparse matrix with columns as word_id & rows as doc_id\n",
    "    ans = apriori(Sp_matrix, min_support=F,use_colnames=True)\n",
    "    result=[]\n",
    "    for i in ans['itemsets']:                    #filtering the frequent itemsets w.r.t set size\n",
    "        if len(i)==K:\n",
    "            result.append(list(i))\n",
    "    vocab_dict = {}                              #creates dictionary to store words corresponding to word_id's\n",
    "    for i in range(len(list(vocab[\"Word\"]))):\n",
    "        vocab_dict[i+1] = list(vocab[\"Word\"])[i]        \n",
    "    final = []\n",
    "    for i in range(len(result)):                 #replacing word_id's by the words itself\n",
    "        temp = []\n",
    "        for j in range(K):\n",
    "            temp.append(vocab_dict[int(result[i][j])])\n",
    "        final.append(temp)\n",
    "    end=time.time()\n",
    "    print(\"The running time of the code is : \",end-start,file=f)    #prints the running time of the code\n",
    "    print(final,file=f) \n",
    "    return (final,end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['friday'],\n",
       "  ['gas'],\n",
       "  ['give'],\n",
       "  ['going'],\n",
       "  ['group'],\n",
       "  ['help'],\n",
       "  ['hope'],\n",
       "  ['hour'],\n",
       "  ['houston'],\n",
       "  ['including'],\n",
       "  ['issue'],\n",
       "  ['issues'],\n",
       "  ['list'],\n",
       "  ['look'],\n",
       "  ['market'],\n",
       "  ['meeting'],\n",
       "  ['message'],\n",
       "  ['attached'],\n",
       "  ['monday'],\n",
       "  ['month'],\n",
       "  ['note'],\n",
       "  ['number'],\n",
       "  ['offer'],\n",
       "  ['office'],\n",
       "  ['order'],\n",
       "  ['part'],\n",
       "  ['phone'],\n",
       "  ['place'],\n",
       "  ['plan'],\n",
       "  ['point'],\n",
       "  ['power'],\n",
       "  ['price'],\n",
       "  ['problem'],\n",
       "  ['process'],\n",
       "  ['provide'],\n",
       "  ['receive'],\n",
       "  ['received'],\n",
       "  ['report'],\n",
       "  ['request'],\n",
       "  ['review'],\n",
       "  ['send'],\n",
       "  ['service'],\n",
       "  ['services'],\n",
       "  ['set'],\n",
       "  ['start'],\n",
       "  ['support'],\n",
       "  ['sure'],\n",
       "  ['team'],\n",
       "  ['thing'],\n",
       "  ['think'],\n",
       "  ['working'],\n",
       "  ['address'],\n",
       "  ['business'],\n",
       "  ['california'],\n",
       "  ['change'],\n",
       "  ['able'],\n",
       "  ['comment'],\n",
       "  ['company'],\n",
       "  ['contract'],\n",
       "  ['corp'],\n",
       "  ['cost'],\n",
       "  ['current'],\n",
       "  ['customer'],\n",
       "  ['date'],\n",
       "  ['deal'],\n",
       "  ['discuss'],\n",
       "  ['due'],\n",
       "  ['end'],\n",
       "  ['energy'],\n",
       "  ['find'],\n",
       "  ['forward'],\n",
       "  ['free']],\n",
       " 215.25746512413025)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Frequent_itemset(\"enron\",1,0.09,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['democratic', 'democrats'],\n",
       "  ['general', 'democratic'],\n",
       "  ['democratic', 'house'],\n",
       "  ['democratic', 'kerry'],\n",
       "  ['democratic', 'poll'],\n",
       "  ['democratic', 'primary'],\n",
       "  ['democratic', 'bush'],\n",
       "  ['bush', 'democrats'],\n",
       "  ['general', 'election'],\n",
       "  ['bush', 'election'],\n",
       "  ['general', 'kerry'],\n",
       "  ['general', 'poll'],\n",
       "  ['general', 'war'],\n",
       "  ['general', 'bush'],\n",
       "  ['bush', 'house'],\n",
       "  ['poll', 'kerry'],\n",
       "  ['kerry', 'primary'],\n",
       "  ['kerry', 'war'],\n",
       "  ['bush', 'kerry'],\n",
       "  ['bush', 'media'],\n",
       "  ['poll', 'bush'],\n",
       "  ['president', 'bush'],\n",
       "  ['bush', 'republicans'],\n",
       "  ['time', 'bush'],\n",
       "  ['bush', 'war'],\n",
       "  ['administration', 'bush']],\n",
       " 65.61700534820557)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Frequent_itemset(\"kos\",2,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['set', 'system'],\n",
       "  ['data', 'set'],\n",
       "  ['set', 'abstract'],\n",
       "  ['function', 'set'],\n",
       "  ['set', 'information'],\n",
       "  ['input', 'set'],\n",
       "  ['set', 'introduction'],\n",
       "  ['learning', 'set'],\n",
       "  ['model', 'set'],\n",
       "  ['network', 'set'],\n",
       "  ['set', 'neural'],\n",
       "  ['number', 'set'],\n",
       "  ['problem', 'set'],\n",
       "  ['references', 'set'],\n",
       "  ['set', 'result'],\n",
       "  ['abstract', 'system'],\n",
       "  ['function', 'system'],\n",
       "  ['information', 'system'],\n",
       "  ['input', 'system'],\n",
       "  ['introduction', 'system'],\n",
       "  ['model', 'system'],\n",
       "  ['network', 'system'],\n",
       "  ['neural', 'system'],\n",
       "  ['number', 'system'],\n",
       "  ['problem', 'system'],\n",
       "  ['references', 'system'],\n",
       "  ['result', 'system'],\n",
       "  ['values', 'abstract'],\n",
       "  ['case', 'abstract'],\n",
       "  ['function', 'case'],\n",
       "  ['case', 'neural'],\n",
       "  ['references', 'case'],\n",
       "  ['case', 'result'],\n",
       "  ['data', 'abstract'],\n",
       "  ['function', 'data'],\n",
       "  ['data', 'neural'],\n",
       "  ['references', 'data'],\n",
       "  ['data', 'result'],\n",
       "  ['algorithm', 'abstract'],\n",
       "  ['references', 'algorithm'],\n",
       "  ['form', 'abstract'],\n",
       "  ['function', 'abstract'],\n",
       "  ['abstract', 'information'],\n",
       "  ['input', 'abstract'],\n",
       "  ['abstract', 'introduction'],\n",
       "  ['large', 'abstract'],\n",
       "  ['learning', 'abstract'],\n",
       "  ['model', 'abstract'],\n",
       "  ['network', 'abstract'],\n",
       "  ['abstract', 'neural'],\n",
       "  ['number', 'abstract'],\n",
       "  ['order', 'abstract'],\n",
       "  ['output', 'abstract'],\n",
       "  ['abstract', 'paper'],\n",
       "  ['abstract', 'parameter'],\n",
       "  ['point', 'abstract'],\n",
       "  ['problem', 'abstract'],\n",
       "  ['processing', 'abstract'],\n",
       "  ['references', 'abstract'],\n",
       "  ['abstract', 'result'],\n",
       "  ['function', 'information'],\n",
       "  ['function', 'input'],\n",
       "  ['function', 'introduction'],\n",
       "  ['learning', 'function'],\n",
       "  ['function', 'model'],\n",
       "  ['function', 'network'],\n",
       "  ['function', 'neural'],\n",
       "  ['function', 'number'],\n",
       "  ['problem', 'function'],\n",
       "  ['function', 'references'],\n",
       "  ['function', 'result'],\n",
       "  ['introduction', 'information'],\n",
       "  ['network', 'information'],\n",
       "  ['neural', 'information'],\n",
       "  ['number', 'information'],\n",
       "  ['references', 'information'],\n",
       "  ['information', 'result'],\n",
       "  ['input', 'introduction'],\n",
       "  ['input', 'network'],\n",
       "  ['input', 'neural'],\n",
       "  ['input', 'number'],\n",
       "  ['references', 'input'],\n",
       "  ['input', 'result'],\n",
       "  ['model', 'introduction'],\n",
       "  ['network', 'introduction'],\n",
       "  ['neural', 'introduction'],\n",
       "  ['number', 'introduction'],\n",
       "  ['problem', 'introduction'],\n",
       "  ['references', 'introduction'],\n",
       "  ['introduction', 'result'],\n",
       "  ['learning', 'neural'],\n",
       "  ['learning', 'references'],\n",
       "  ['learning', 'result'],\n",
       "  ['model', 'network'],\n",
       "  ['model', 'neural'],\n",
       "  ['model', 'number'],\n",
       "  ['references', 'model'],\n",
       "  ['model', 'result'],\n",
       "  ['network', 'neural'],\n",
       "  ['network', 'number'],\n",
       "  ['problem', 'network'],\n",
       "  ['references', 'network'],\n",
       "  ['network', 'result'],\n",
       "  ['number', 'neural'],\n",
       "  ['problem', 'neural'],\n",
       "  ['references', 'neural'],\n",
       "  ['neural', 'result'],\n",
       "  ['problem', 'number'],\n",
       "  ['references', 'number'],\n",
       "  ['number', 'result'],\n",
       "  ['references', 'order'],\n",
       "  ['order', 'result'],\n",
       "  ['references', 'paper'],\n",
       "  ['paper', 'result'],\n",
       "  ['problem', 'references'],\n",
       "  ['problem', 'result'],\n",
       "  ['references', 'result']],\n",
       " 100.16626214981079)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Frequent_itemset(\"nips\",2,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
